# -*- coding: utf-8 -*-
"""PruebaDD360_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18WB-n5egtDPPZMNPqLte0TR8TnFcjm-x

# Reto / Instrucciones

Como un científico de datos, tu misión es predecir el tiempo que una propiedad está en el mercado. Establece la métrica con la que vas a evaluar dicha predicción. ¿Cómo se ve tu análisis para las propiedades que aún no se venden contra las que ya se vendieron?

- Nota: El tiempo que una propiedad está en el mercado está en la columna time2event, el cual está medido en días, y si se vendió o no la propiedad está en event, marcado con 1 o 0, respectivamente.
- El archivo se llama liquidity_challenge

---

**Para un resumen de lo que se realizo ve a la sección 6) Análisis de Resultados y Conclusiones**

---


# **Descripcion de las tareas que voy a realizar**

Para predecir el tiempo que una propiedad permanece en el mercado antes de venderse, usaré técnicas de análisis de supervivencia, ya que este enfoque es adecuado para manejar datos censurados (propiedades que aún no se han vendido) y eventos (ventas de propiedades).

Los pasos que seguiré son los siguientes:

1. **Preprocesamiento de Datos**:
  - Esto incluye la limpieza de los datos, el manejo de valores faltantes, la transformación de variables categóricas y la normalización de variables numéricas.

2. **Análisis Exploratorio de Datos:**
  - Examinar las características de las propiedades vendidas frente a las no vendidas para entender las diferencias.

3. **Modelado de Supervivencia:**
  - Utilizar modelos de supervivencia como el modelo de Cox, Random Survival Forests, y Gradient Boosting para supervivencia. Estos modelos son adecuados para manejar la censura en los datos.

4. **Evaluación y Comparación de los Modelos:**
  - El rendimiento de los modelos se evaluará utilizando el índice de concordancia (C-index). Este índice es una medida de la capacidad predictiva del modelo, donde un valor más alto indica un mejor rendimiento.
  - Comparar los modelos en base a su índice de concordancia y seleccionar el más adecuado.

5. **Importancia de Características:**
  - Analizar cuáles características son más importantes para predecir el tiempo en el mercado utilizando técnicas como SHAP (SHapley Additive exPlanations).

6. **Análisis de Resultados y Conclusiones:**
  - *Interpretar los resultados del modelo en el contexto del negocio inmobiliario, proporcionando insights sobre qué factores influyen más en el tiempo que una propiedad permanece en el mercado.*

7. (Extra) **Modelo de venta**:
  - Crear un modelo que pueda predecir la venta o no de la propiedad de acuerto con los atributos proporcionados y analizar la importancia de estos atributos que lleven a una probabilidad mas alta de venta.



---

---


# **1) Preprocesamiento de Datos**

En esta seccion voy a trabajar en la limpieza de los datos, el manejo de valores faltantes, la transformación de variables categóricas y la normalización de variables numéricas.

## Imports and packages
"""

# Montando Google Drive
from google.colab import drive
drive.mount('/content/drive')

!pip install scikit-survival

!pip install lifelines

!pip install shap

!pip install --upgrade xgboost

# Commented out IPython magic to ensure Python compatibility.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import sklearn

from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sksurv.ensemble import RandomSurvivalForest
from sklearn.model_selection import GridSearchCV
from sksurv.metrics import concordance_index_censored
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint
from sksurv.linear_model import CoxPHSurvivalAnalysis
from lifelines import CoxPHFitter

from sksurv.ensemble import GradientBoostingSurvivalAnalysis

import xgboost as xgb

import shap

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# from geopy.geocoders import Nominatim

import warnings
warnings.filterwarnings("ignore")
warnings.filterwarnings(action='ignore', category=FutureWarning)
import matplotlib.font_manager as fm

# %matplotlib inline

"""## Data

El primer paso es cargar la data en un dataframe para poder explorarlo.
"""

# Cargando la data
file_path = '/content/drive/MyDrive/Colab Notebooks/OtherNotebooks/DD360/liquidity_challenge.xlsx'
df = pd.read_excel(file_path)

# Análisis inicial del DataFrame
df_info = df.info()
df_describe = df.describe()
df.head()

print("\nDescripcion del dataset: \n", df_describe)

"""## Preprocessing"""

# Cargando la data
file_path = '/content/drive/MyDrive/Colab Notebooks/OtherNotebooks/DD360/liquidity_challenge.xlsx'
df = pd.read_excel(file_path)

df = df.copy()

# Calcular y agregar el precio por metro cuadrado
df['price_per_sqm'] = df['last_price'] / df['surface_total']

var_drop = ['property_id'] # Suponemos que no son relevantes para el analisis del caso
df.drop(var_drop, axis=1, inplace=True)

# Copias del DataFrame que usaré mas adelante
df_analysis = df
df_prediction = df

#########

# Identifica las características numéricas y categóricas
numeric_features = df.select_dtypes(include=['int64', 'float64']).drop(['event', 'time2event'], axis=1).columns
categorical_features = df.select_dtypes(include=['object']).columns

# Crea transformadores para preprocesamiento
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

# Combina el preprocesamiento para numéricos y categóricos
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

###########

# Excluye de columnas no utilizadas para la predicción (y)
X = df.drop(['event', 'time2event'], axis=1)
y = df[['time2event', 'event']]  # Objetivo para análisis de supervivencia

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)

##########

# Recupera los nombres de características después de la codificación one-hot
ohe_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)
feature_names = numeric_features.tolist() + ohe_feature_names.tolist()

# Crea los DataFrames con las características preprocesadas
X_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, columns=feature_names)
X_test_preprocessed_df = pd.DataFrame(X_test_preprocessed, columns=feature_names)

# Verifico que las matrices de los datos sean del mismo tamaño
print("\n X_train shape: ", X_train_preprocessed_df.shape)
print(" y_train shape: ", y_train.shape)

print("\n X_test shape: ", X_test_preprocessed_df.shape)
print(" y_test shape: ", y_test.shape)

"""

---


# **2) Analysis de Datos**
"""

# Divido el DataFrame en dos grupos: propiedades vendidas y no vendidas
sold_properties = df_prediction[df_prediction['event'] == 1]
unsold_properties = df_prediction[df_prediction['event'] == 0]

# Calcula estadísticas descriptivas para cada grupo
sold_stats = sold_properties.describe()
unsold_stats = unsold_properties.describe()

# Calcula el promedio de tiempo en el mercado para cada grupo
average_time_sold = sold_properties['time2event'].mean()
average_time_unsold = unsold_properties['time2event'].mean()

# Separación de propiedades vendidas y no vendidas
sold = df[df['event'] == 1]
not_sold = df[df['event'] == 0]

# Análisis de precios por metro cuadrado
mean_price_sold = sold['price_per_sqm'].mean()
mean_price_not_sold = not_sold['price_per_sqm'].mean()

# Análisis del número de habitaciones
mean_rooms_sold = sold['num_bedrooms'].mean()
mean_rooms_not_sold = not_sold['num_bedrooms'].mean()

# Análisis de la superficie total
mean_surface_sold = sold['surface_total'].mean()
mean_surface_not_sold = not_sold['surface_total'].mean()

print("\n Tiempo promedio en el mercado de propiedades vendidas: ", average_time_sold)
print("\n Tiempo promedio en el mercado de propiedades no vendidas: ", average_time_unsold)

print("\n \n Estadisticas propiedades vendidas: \n")
sold_stats

print("\n \n Estadisticas propiedades no vendidas: \n")
unsold_stats

# Event: Propiedades vendidas y en venta
y_counts = df['event'].value_counts(normalize=True).round(2) * 100
print("Propiedades vendidas y en venta")
print(y_counts)
# labels = y_counts.index
labels = ['vendidas', 'en venta']
plt.pie(y_counts, startangle=90)
plt.legend(labels, loc='best',fontsize=15)
plt.title("Event: Propiedades vendidas y en venta",size=22)

# Configura el estilo de las gráficas
sns.set(style="whitegrid")

# Crea la figura de tamaño adecuado
plt.figure(figsize=(14, 6))

# Crea un histograma para las propiedades vendidas
plt.subplot(1, 2, 1)
sns.histplot(sold_properties['time2event'], bins=30, kde=False, color='blue')
plt.title('Distribución del tiempo en el mercado para propiedades vendidas')
plt.xlabel('Días en el mercado')
plt.ylabel('Cantidad de propiedades')

# Crea un histograma para las propiedades no vendidas
plt.subplot(1, 2, 2)
sns.histplot(unsold_properties['time2event'], bins=30, kde=False, color='red')
plt.title('Distribución del tiempo en el mercado para propiedades no vendidas')
plt.xlabel('Días en el mercado')
plt.ylabel('Cantidad de propiedades')

# Muestra las graficas
plt.tight_layout()
plt.show()

# Crea un diagrama de cajas para comparar las dos distribuciones
plt.figure(figsize=(8, 6))
sns.boxplot(x='event', y='time2event', data=df, palette='Set2')
plt.title('Comparación del tiempo en el mercado para propiedades vendidas vs no vendidas')
plt.xlabel('Evento (1: Vendida, 0: No Vendida)')
plt.ylabel('Días en el mercado')
plt.show()

# Distribución de la superficie total
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
sns.boxplot(x='event', y='surface_total', data=df)
plt.title('Distribución de la Superficie Total por Estado de Venta')
plt.xlabel('Evento (1: Vendido, 0: No Vendido)')
plt.ylabel('Superficie Total')

# Distribución del número de habitaciones
plt.subplot(1, 2, 2)
sns.boxplot(x='event', y='num_bedrooms', data=df)
plt.title('Distribución del Número de Habitaciones por Estado de Venta')
plt.xlabel('Evento (1: Vendido, 0: No Vendido)')
plt.ylabel('Número de Habitaciones')

plt.tight_layout()
plt.show()

# Precio por metro cuadrado
plt.figure(figsize=(12, 6))
sns.boxplot(x='event', y='price_per_sqm', data=df)
plt.title('Precio por Metro Cuadrado por Estado de Venta')
plt.xlabel('Vendido (1) vs No Vendido (0)')
plt.ylabel('Precio por Metro Cuadrado')
plt.show()

g = sns.FacetGrid(df, col="event", hue="event", palette="Set1", height=6)
g.map(sns.histplot, "price_per_sqm")
g.add_legend()
g.set_axis_labels("Precio por Metro Cuadrado", "Cantidad")
g.set_titles("No Vendido {col_name}")
plt.show()

# 1. Distribución de Precios por Metro Cuadrado
g1 = sns.FacetGrid(df, col="event", height=4, aspect=1.5)
g1.map(sns.histplot, "price_per_sqm")
g1.set_axis_labels("Precio por Metro Cuadrado", "Cantidad")
g1.set_titles("Vendido: {col_name}")

# 2. Relación entre Superficie Total y Precio
g2 = sns.FacetGrid(df, col="event", height=4, aspect=1.5)
g2.map(sns.scatterplot, "surface_total", "last_price")
g2.set_axis_labels("Superficie Total", "Precio")
g2.set_titles("Vendido: {col_name}")

# 3. Número de Habitaciones y Precio
g3 = sns.FacetGrid(df, col="event", height=4, aspect=1.5)
g3.map(sns.boxplot, "num_bedrooms", "last_price")
g3.set_axis_labels("Número de Habitaciones", "Precio")
g3.set_titles("Vendido: {col_name}")

# 4. Tipo de Propiedad y Precio
g4 = sns.FacetGrid(df, col="event", height=4, aspect=1.5)
g4.map(sns.boxplot, "property_type", "last_price")
g4.set_axis_labels("Tipo de Propiedad", "Precio")
g4.set_titles("Vendido: {col_name}")

# 5. Distribución del Tiempo en el Mercado
g5 = sns.FacetGrid(df, col="event", height=4, aspect=1.5)
g5.map(sns.histplot, "time2event")
g5.set_axis_labels("Tiempo en el Mercado (días)", "Cantidad")
g5.set_titles("Vendido: {col_name}")

# Mostrar gráficos
plt.show()

# 3. Número de visitas
plt.figure(figsize=(12, 6))
sns.countplot(x='views', hue='event', data=df)
plt.title('Número de Visitas por Estado de Venta')
plt.xlabel('Número de Visitas')
plt.ylabel('Cantidad')
plt.show()

# 4. Costo de la propiedad
plt.figure(figsize=(12, 6))
sns.boxplot(x='event', y='last_price', data=df)
plt.title('Costo de la Propiedad por Estado de Venta')
plt.xlabel('Vendido (1) vs No Vendido (0)')
plt.ylabel('Costo de la Propiedad')
plt.show()

# 5. Tipo de propiedad
plt.figure(figsize=(12, 6))
sns.countplot(x='property_type', hue='event', data=df)
plt.title('Tipo de Propiedad por Estado de Venta')
plt.xlabel('Tipo de Propiedad')
plt.ylabel('Cantidad')
plt.show()

# 6. Tamaño de la propiedad (superficie total)
plt.figure(figsize=(12, 6))
sns.boxplot(x='event', y='surface_total', data=df)
plt.title('Tamaño de la Propiedad por Estado de Venta')
plt.xlabel('Vendido (1) vs No Vendido (0)')
plt.ylabel('Superficie Total')
plt.show()

# 7. Número de marketplaces
plt.figure(figsize=(12, 6))
sns.countplot(x='num_marketplaces_property_at', hue='event', data=df)
plt.title('Número de Marketplaces por Estado de Venta')
plt.xlabel('Número de Marketplaces')
plt.ylabel('Cantidad')
plt.show()

# Calcula la matriz de correlación para las propiedades vendidas y no vendidas
correlation_matrix_sold = sold_properties.corr()
correlation_matrix_unsold = unsold_properties.corr()

# Crea una figura de tamaño adecuado para el mapa de calor
plt.figure(figsize=(15, 10))

# Mapa de calor para las correlaciones de las propiedades vendidas
plt.subplot(1, 2, 1)
sns.heatmap(correlation_matrix_sold, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Mapa de calor de correlaciones para propiedades vendidas')

# Mapa de calor para las correlaciones de las propiedades no vendidas
plt.subplot(1, 2, 2)
sns.heatmap(correlation_matrix_unsold, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Mapa de calor de correlaciones para propiedades no vendidas')

plt.tight_layout()
plt.show()

# Calcula la matriz de correlación para las características numéricas
correlation_matrix = df.corr()

# Visualiza la matriz de correlación con respecto a 'time2event'
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix[['time2event']].sort_values(by='time2event', ascending=False),
            annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt=".2f")
plt.title('Correlación de características con el tiempo en el mercado')
plt.show()

# Selecciona algunas características numéricas para los scatter plots
selected_features = ['surface_total', 'num_bathrooms', 'num_bedrooms', 'num_parking_lots']

# Crea scatter plots
plt.figure(figsize=(20, 15))

for i, feature in enumerate(selected_features, 1):
    plt.subplot(2, 2, i)
    sns.scatterplot(data=df, x=feature, y='time2event', hue='event', alpha=0.6)
    plt.title(f'Tiempo en el mercado vs {feature}')
    plt.xlabel(f'{feature}')
    plt.ylabel('Días en el mercado')
    plt.legend(title='Propiedad vendida')

plt.tight_layout()
plt.show()

# Analiza la relación entre el tamaño de la propiedad y el tiempo en el mercado
# También se considero si se vendió o no
sns.scatterplot(data=df, x='surface_total', y='time2event', hue='event')
plt.title('Tamaño de la Propiedad vs. Tiempo en el Mercado')
plt.xlabel('Superficie Total (m²)')
plt.ylabel('Tiempo en el Mercado (días)')
plt.show()

# Analiza el precio por metro cuadrado contra el tiempo en el mercado
sns.scatterplot(data=df, x='price_per_sqm', y='time2event')
plt.title('Precio por Metro Cuadrado vs. Tiempo en el Mercado')
plt.xlabel('Precio por Metro Cuadrado')
plt.ylabel('Tiempo en el Mercado (días)')
plt.show()

# Analiza el número de habitaciones contra el tiempo en el mercado
sns.boxplot(data=df, x='num_bedrooms', y='time2event')
plt.title('Número de Habitaciones vs. Tiempo en el Mercado')
plt.xlabel('Número de Habitaciones')
plt.ylabel('Tiempo en el Mercado (días)')
plt.show()

"""---



# **3) Modelado de Supervivencia**

Para predecir el tiempo que una propiedad está en el mercado, la métrica más adecuada para evaluar la predicción sería el índice de concordancia, conocido también como el índice C. Esta métrica es comúnmente utilizada en análisis de supervivencia y modelos de riesgo proporcional como los mencionados (Cox, Random Survival Forest, etc.). El índice de concordancia mide la capacidad del modelo para clasificar correctamente pares de sujetos en términos de sus tiempos de evento. Un valor de 1 indica una predicción perfecta, mientras que un valor de 0.5 sugiere un rendimiento no mejor que el azar.

**En cuanto al análisis de las propiedades que aún no se han vendido (censuradas) frente a las que ya se vendieron**:

 - *Propiedades Vendidas (event = 1)*: Para estas propiedades, el tiempo hasta la venta (time2event) es conocido y se puede utilizar directamente para entrenar y evaluar el modelo. El modelo puede aprender de estos casos para identificar patrones y correlaciones que influyen en el tiempo de venta.

 - *Propiedades No Vendidas (event = 0)*: Estas propiedades son censuradas, lo que significa que no sabemos cuándo (o si) se venderán. En el análisis de supervivencia, estos casos son tratados de manera especial. Aunque el tiempo exacto de venta no se conoce, la información hasta la fecha de censura aún aporta valor al análisis. El modelo puede utilizar esta información para entender las características de las propiedades que tienden a permanecer más tiempo en el mercado.

Utilizaré cuatro modelos diferentes para manejar datos censurados:
1. Random Survival Forests
2. Modelo de riesgos proporcionales de Cox
3. Gradient Boosting for Survival Analysis.
4. XGBoost

Compararé su rendimiento utilizando métricas como el Concordance index o Índice de concordancia.

## Random Survival Forest (RSF)
"""

# Converte y_train y y_test a un formato estructurado para análisis de supervivencia
structured_y_train = np.array([(bool(event), time) for time, event in y_train.itertuples(index=False)],
                              dtype=[('event', bool), ('time2event', float)])
structured_y_test = np.array([(bool(event), time) for time, event in y_test.itertuples(index=False)],
                             dtype=[('event', bool), ('time2event', float)])

# Ajusta el modelo RSF
rsf_model = RandomSurvivalForest(n_estimators=100, min_samples_split=10, random_state=42)
rsf_model.fit(X_train_preprocessed_df, structured_y_train)

# Evalua el modelo
rsf_prediction = rsf_model.predict(X_test_preprocessed_df)
rsf_ci = concordance_index_censored(structured_y_test['event'], structured_y_test['time2event'], rsf_prediction)

print("RSF Índice de concordancia en el test set:", rsf_ci[0])

# Crea una instancia del modelo RSF
rsf = RandomSurvivalForest(random_state=42)

# Define la cuadrícula de hiperparámetros a probar
param_dist = {
    'n_estimators': randint(50, 200),  # Rango más pequeño y distribución aleatoria
    'max_depth': [None, 10, 20],       # Menos opciones
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2']
}

# Configura la búsqueda aleatoria
random_search = RandomizedSearchCV(estimator=rsf, param_distributions=param_dist,
                                   n_iter=10, cv=5, n_jobs=-1, random_state=42, scoring='roc_auc')

# Ajusta la búsqueda aleatoria al conjunto de entrenamiento
random_search.fit(X_train_preprocessed_df, structured_y_train)

# Mejores hiperparámetros encontrados
best_params = random_search.best_params_
print("\nMejores hiperparámetros encontrados:\n", best_params)

# Evalua el modelo con los mejores hiperparámetros
best_rsf = random_search.best_estimator_
rsf_prediction = best_rsf.predict(X_test_preprocessed_df)
rsf_ci = concordance_index_censored(structured_y_test['event'], structured_y_test['time2event'], rsf_prediction)

print("\nRSF Índice de concordancia en el test set con mejores hiperparámetros:", rsf_ci[0])

"""## Modelo de Cox

El Modelo de Cox, es un método estadístico que se usa para la investigación de supervivencia. Su propósito principal es investigar la relación entre el tiempo de supervivencia de los sujetos y una o más variables predictoras.

.

En este contexto del análisis de supervivencia, "supervivencia" se refiere al evento de la venta de una propiedad.
"""

# Prepara el DataFrame de lifelines uniendo X y y
lifelines_train_df = X_train_preprocessed_df.join(y_train.reset_index(drop=True))

# Viendo de que las columnas 'time2event' y 'event' están presentes
assert 'time2event' in lifelines_train_df.columns and 'event' in lifelines_train_df.columns

# Ajuste del modelo de Cox
cox_model = CoxPHFitter(penalizer=0.1)
cox_model.fit(lifelines_train_df, duration_col='time2event', event_col='event')

# Prepara el DataFrame de prueba
lifelines_test_df = X_test_preprocessed_df.join(y_test.reset_index(drop=True))

# Veo de que las columnas 'time2event' y 'event' están presentes en el DataFrame de prueba
assert 'time2event' in lifelines_test_df.columns and 'event' in lifelines_test_df.columns

# Evaluación del modelo
cox_concordance_index = cox_model.score(lifelines_test_df, scoring_method="concordance_index")

print("Índice de concordancia en el test set:", cox_concordance_index)

"""## Gradient Boosting Survival Analysis

El Gradient Boosting es una técnica de aprendizaje automático que construye un modelo predictivo de manera iterativa, añadiendo árboles de decisión que corrigen los errores de las predicciones anteriores.
"""

# Crea y ajustar el modelo
gb_model = GradientBoostingSurvivalAnalysis(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)
gb_model.fit(X_train_preprocessed_df, structured_y_train)

# Evalua el modelo
gb_prediction = gb_model.predict(X_test_preprocessed_df)
gb_ci = concordance_index_censored(structured_y_test['event'], structured_y_test['time2event'], gb_prediction)

print("Gradient Boosting Índice de concordancia en el test set:", gb_ci[0])

"""## XGBoost

XGBoost no soporta directamente datos censurados, pero puedo adaptarlo para análisis de supervivencia utilizando el enfoque de Cox: transformar los datos de supervivencia para que se ajusten a un modelo de regresión.
"""

# La etiqueta para la regresión de supervivencia es el tiempo hasta el evento
dtrain = xgb.DMatrix(X_train_preprocessed_df, label=[t[1] for t in structured_y_train])
dtest = xgb.DMatrix(X_test_preprocessed_df, label=[t[1] for t in structured_y_test])

# Configuración de parámetros para XGBoost
# Usare 'survival:cox' para el análisis de supervivencia
params = {
    'max_depth': 3,
    'eta': 0.1,
    'objective': 'survival:cox'
}
num_round = 50  # Número de rondas de entrenamiento

# Entrena el modelo XGBoost
bst = xgb.train(params, dtrain, num_round)

# Hace predicciones
preds = bst.predict(dtest)

# Calcula el índice de concordancia
ci_xgboost = concordance_index_censored(structured_y_test['event'], structured_y_test['time2event'], preds)
print("XGBoost Concordance Index:", ci_xgboost[0])

"""---



# **4) Evaluación y Comparación de los Modelos**

Los índices de concordancia que obtuvimos para los distintos modelos de supervivencia nos proporcionan una valiosa perspectiva sobre su capacidad predictiva en relación con nuestro objetivo: p*redecir el tiempo que una propiedad permanece en el mercado*. El índice de concordancia (C-index) es una medida de cuán bien el modelo puede distinguir entre pares de individuos en términos de sus tiempos de evento. Un valor más alto indica una mejor capacidad predictiva.

## Resultados de los diferentes modelos de supervivencia:
"""

print("Índice de concordancia de los modelos: \n")
print("RSF:", rsf_ci[0]) # RSF
print("Modelo de Cox (lifelines):", cox_concordance_index) #lifelines
print("Gradient Boosting:", gb_ci[0]) #GBS
print("XGBoost:", ci_xgboost[0]) #XGBoost

"""Los resultados de los diferentes modelos de supervivencia son:

- **Modelo de Cox (lifelines)**: Mostró un rendimiento ligeramente mejor que la versión de scikit-survival (quite el codigo porque me daba problemas), con un índice de concordancia de **0.6055**. Esto indica una consistencia en el rendimiento del modelo de Cox, independientemente de la implementación.

- **Random Survival Forest (RSF)**: Este modelo no ajustado obtuvo un índice de concordancia de **0.6207**, el más alto entre los modelos probados inicialmente. Esto sugiere una buena capacidad para predecir el orden de los eventos.

- **Random Survival Forest Ajustado**: Después de la optimización de hiperparámetros, el RSF ajustado logró un índice de concordancia de **0.6243**, que es ligeramente inferior al RSF no ajustado. Esto puede deberse a la variabilidad inherente en los datos o a la especificidad de la configuración de hiperparámetros seleccionada.

- **Gradient Boosting**: Con un índice de concordancia de **0.6209**, el modelo de Gradient Boosting mostró un rendimiento comparable al de RSF, destacando su eficacia en el análisis de supervivencia.
"""

# Resultados de los diferentes modelos
model_results = {
    "Cox (lifelines)": 0.6055,
    "Random Survival Forest": 0.6207,
    "Random Survival Forest (Tuned)": 0.6203,
    "Gradient Boosting": 0.6209,
    "XGBoost": 0.6139
}

# Crea un gráfico de barras para visualizar los resultados
plt.figure(figsize=(10, 6))
plt.bar(model_results.keys(), model_results.values(), color='skyblue')
plt.xlabel('Modelo')
plt.ylabel('Índice de Concordancia')
plt.title('Comparación del Rendimiento de Diferentes Modelos de Supervivencia')
plt.ylim(0.6, 0.63)
plt.xticks(rotation=60)
plt.grid(axis='y')

# Mostrar el gráfico
plt.show()

"""La visualización en forma de gráfico de barras muestra claramente que, aunque hay diferencias en el rendimiento entre los modelos, todos logran índices de concordancia por encima de 0.60, lo que indica una capacidad predictiva mejor que el azar (0.5).

.

Para mejorar el rendimiento, podríamos considerar ajustar los hiperparámetros de los modelos, utilizar técnicas de selección de características para identificar y retener las características más informativas, y explorar modelos más complejos o enfoques híbridos. Además, la incorporación de más datos, especialmente aquellos que puedan capturar tendencias del mercado inmobiliario, podría mejorar la precisión de las predicciones.

---



# **5) Importancia de las Características**

Use SHAP (SHapley Additive exPlanations) para el analisis de los atributos mas importantes. SHAP es una técnica avanzada de interpretación de modelos de aprendizaje automático. Esta herramienta ayuda a entender cómo las características individuales de un conjunto de datos contribuyen a las predicciones de un modelo, proporcionando una explicación detallada y comprensible de estas predicciones.

Los pasos que segui fueron:

1. Entrené un Modelo: Primero, entrené un modelo de Machine Learning, como un modelo de Gradient Boosting utilizando XGBoost, para predecir un evento de interés (en este caso, si una propiedad se vendió o no).

2. Aplicación de SHAP: Después de entrenar el modelo, utilicé SHAP para interpretarlo. SHAP calculó valores para cada característica en cada predicción, indicando cuánto contribuyó cada característica al resultado de la predicción.

3. Visualizaciones de SHAP:

  - Summary Plot: Me proporcionó una vista general de la importancia de las características y de cómo afectaban las predicciones. Mostró tanto el impacto (positivo o negativo) como la importancia de las características en el conjunto de datos.
  - Dependence Plot: Exploré la relación entre una característica específica y su impacto en las predicciones.
"""

# Inicializa el objeto SHAP Explainer con tu modelo XGBoost
explainer = shap.Explainer(bst)

# Calcula los valores SHAP para el conjunto de datos de prueba
shap_values = explainer.shap_values(X_test_preprocessed_df)

# Visualiza la importancia de las características
shap.summary_plot(shap_values, X_test_preprocessed_df)

# Dependence plot para una característica específica
shap.dependence_plot("views", shap_values, X_test_preprocessed_df)

# Dependence plot para una característica específica
shap.dependence_plot("price_per_sqm", shap_values, X_test_preprocessed_df)

# Dependence plot para una característica específica
shap.dependence_plot("marketplace_hash", shap_values, X_test_preprocessed_df)

"""---


# **6) Análisis de Resultados y Conclusiónes**

Después de analizar el conjunto de datos, he encontrado algunas diferencias clave entre las propiedades que se vendieron y las que no.

1. **Precio por Metro Cuadrado:** Observé una diferencia en el precio por metro cuadrado entre las propiedades vendidas y no vendidas. En promedio, el precio por metro cuadrado de las propiedades vendidas fue de aproximadamente \$ 45493  mientras que para las no vendidas fue de \$ 49040. Esto se ilustra en la gráfica que creé abajo, mostrando una clara distinción en la distribución de precios por metro cuadrado entre las propiedades vendidas y no vendidas.

2. **Número de Habitaciones**: En cuanto al número promedio de habitaciones, encontré que las propiedades vendidas tenían en promedio 2.5 habitaciones, mientras que las propiedades no vendidas tenían 2.6. Esto sugiere que las propiedades con cierto número un poco mas bajo de habitaciones pueden tener más demanda.

3. **Superficie Total**: Respecto a la superficie total, las propiedades vendidas tenían en promedio 168 metros cuadrados, comparado con 182 metros cuadrados para las no vendidas. Esto indica que el tamaño de la propiedad influye en su atractivo para los compradores.

4. **Tiempo promedio en el mercado**:  
  - Tiempo promedio en el mercado de propiedades vendidas:  57 días
  - Tiempo promedio en el mercado de propiedades no vendidas:  76 días

5. **Comparacion de modelos**: Hay diferencias en el rendimiento entre los diferentes modelos probados. Pero, todos logran índices de concordancia por encima de 0.60, lo que indica una capacidad predictiva mejor que el azar (0.5).

6. **Importancia de los atributos**: Los atributos mas importantes para calcular el tiempo de una propiedad en el mercado segun el analisis Shap fueron (en orden):
  - *marketplace_hash, num_marketplaces_property_at, seller_hash, views, price_per_sqm, surface_total y ubicacion*.

.


**Relevancia Respecto al Reto Planteado**

En relación con mi reto de predecir el tiempo en el mercado de las propiedades, mis análisis y los resultados obtenidos con los modelos muestran que tengo herramientas razonablemente buenas para distinguir entre propiedades que se venderán rápidamente y aquellas que podrían tardar más en venderse. Sin embargo, reconozco que hay margen de mejora. Un índice de concordancia cercano a 0.62, aunque es mejor que una elección al azar, todavía permite espacio para errores en las predicciones.


Para aplicar estos resultados de manera práctica y estimar el tiempo en el mercado de las propiedades, estoy considerando usar las predicciones de riesgo relativo de los modelos para *clasificar las propiedades en categorías como "venta rápida", "venta moderada" y "venta lenta"*. Aunque, por ahora, no puedo determinar un número exacto de días que una propiedad estará en el mercado, sí puedo categorizarlas basándome en sus puntuaciones de riesgo relativo.
"""

# @title
plt.figure(figsize=(12, 6))
sns.boxplot(x='event', y='price_per_sqm', data=df)
plt.title('Precio por Metro Cuadrado por Estado de Venta')
plt.xlabel('Vendido (1) vs No Vendido (0)')
plt.ylabel('Precio por Metro Cuadrado')
plt.show()

# @title
plt.figure(figsize=(10, 6))
plt.bar(model_results.keys(), model_results.values(), color='skyblue')
plt.xlabel('Modelo')
plt.ylabel('Índice de Concordancia')
plt.title('Comparación del Rendimiento de Diferentes Modelos de Supervivencia')
plt.ylim(0.6, 0.63)
plt.xticks(rotation=60)
plt.grid(axis='y')
plt.show()

shap.summary_plot(shap_values, X_test_preprocessed_df)

"""**Aplicación Práctica y Estimación del Tiempo en el Mercado**

- Para amarrar estos resultados a la predicción del tiempo exacto que una propiedad estará en el mercado, podemos usar las predicciones de riesgo relativo de nuestros modelos para clasificar las propiedades en categorías de "venta rápida", "venta moderada" y "venta lenta". Aunque no podemos determinar un número exacto de días, sí podemos categorizar las propiedades en estos segmentos basándonos en sus puntuaciones de riesgo relativo.

- Por ejemplo, podríamos analizar los datos históricos para establecer umbrales de tiempo que definan estas categorías y luego usar las predicciones de nuestro modelo para asignar cada propiedad a una de ellas. Esta aproximación nos permite brindar estimaciones prácticas que pueden ser de gran utilidad para estrategias de marketing y toma de decisiones en el mercado inmobiliario.

**Insights y Recomendaciones**

- *Estrategia de Precios*: Es crucial establecer precios competitivos basados en el valor del mercado. Las propiedades con precios por metro cuadrado alineados con las expectativas del mercado tienden a venderse más rápidamente.

- *Target de Habitaciones*: Las propiedades con un número específico de habitaciones parecen ser más populares, lo cual debe ser considerado en la estrategia de marketing y posicionamiento.

- *Tamaño Importa*: El tamaño de la propiedad juega un papel importante en su venta. Propiedades de tamaños específicos pueden atraer a más compradores.

---


# **7) Modelo para predecir la venta (EXTRA)**
"""

df_prediction.head()

# Transforma fechas a timestamp y variables categóricas a dummies
df_prediction['first_observed_date'] = pd.to_datetime(df_prediction['first_observed_date']).astype(int) / 10**9
df_prediction['last_observed_date'] = pd.to_datetime(df_prediction['last_observed_date']).astype(int) / 10**9
df_prediction = pd.get_dummies(df_prediction, columns=['property_type'])

# Trata valores NaN
nan_columns = ['num_bathrooms', 'num_bedrooms', 'num_parking_lots']
for col in nan_columns:
    median_value = df_prediction[col].median()
    df_prediction[col].fillna(median_value, inplace=True)

# Divide los datos para entrenamiento y prueba
X = df_prediction.drop('event', axis=1)
y = df_prediction['event']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Entrena el modelo
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evalua el modelo
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Accuracy del modelo Random Forest: ", accuracy)
print("\nReporte: \n", report)

"""Precisión: 99.79%

Informe de clasificación:

  - Precisión, recuperación (recall) y puntuación F1 son todas cercanas o iguales a 1.00 para ambas clases (0 y 1), lo que indica un rendimiento muy alto.

Este alto rendimiento es muy bueno, **pero también podría indicar un sobreajuste (overfiting)**. Sería prudente mas adelante realizar una validación cruzada o probar el modelo con un conjunto de datos de validación independiente para confirmar su capacidad de generalización.
"""

# Obteniendo la importancia de las características (feature importance) del modelo entrenado
feature_importance = model.feature_importances_

# Creando un DataFrame para una mejor visualización
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})
feature_importance_df.sort_values(by='Importance', ascending=False, inplace=True)

print("Las características más importantes del modelo RandomForest según su importancia son las siguientes: \n")
feature_importance_df.head(10)  # Mostrando las 10 características más importantes

"""Esto indica que las fechas de observación (tanto la primera como la última) y el tiempo hasta el evento son los factores más influyentes en la predicción del modelo. Las características relacionadas con el precio y la ubicación también tienen cierta relevancia, aunque mucho menor en comparación.

# Exportar notebook
"""

!pip install nbconvert

!jupyter nbconvert --to html PruebaDD360_2.ipynb

